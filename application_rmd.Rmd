---
title: "wowa_analysis_application"
author: "Alexandru Craevschi"
date: "30 01 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ape)
library(cmdstanr)
library(data.table)
library(fdrtool)
library(ggcorrplot)
library(ggplot2)
library(gmapsdistance)
library(glottoTrees)
library(patchwork)
library(phyloWeights)
library(tidyverse)
library(rethinking)

```

The model is based on Word Order in Western Asia (WOWA) dataset, which can be found at https://multicast.aspra.uni-bamberg.de/resources/wowa/. The interesting question behind word order in Western Asia is that many of non-subject elements in the languages of Western Asia have certain degree of flexibility with respect to their position relative to the verb (pre-verbal vs. post-verbal). Apart from that, depending on the role, some elements occupy pre-verbal position and some the post-verbal one. The work is interesting as it works with the corpora, instead of classical discrete categories and thus can capture variability in different roles' position not only cross-linguistically but also intralinguistically. In the term paper, I am most interested in modelling the choice between pre-verbal and post-verbal position and estimating the relative importance of different structural, phylogenetic and geographical factors on this choice. 

Please note that this work is yet unfinished and the whole analysis will include more things, such as, for example, the comparison of model with phylogenetic distance and Gaussian process model vs. multilevel model where pooling would be based on doculect's clade (affiliation2 variable in the dataset) with the use of WAIC and/or leave-one-out cross validation.

In the first part, I compute the geographical distance between the locations where the doculects are spoken. For that, I decided to use walking distance from Google Maps, as it nicely models the way people tend to move. Apparently, throughout the time, some of the roads were nonexistent and people might have migrated somewhere else but overall this metric is more realistic than distance "as the crow flies". Also note that the modern roads often follow well established paths in the earlier periods of the history. For language like Kumzari (coded "musandam" in the dataset), Google Maps allows the use of ferry to shorten the walking distance. At the same time, geographical objects like mountains have to be avoided to get to a different point. These are some of the reasons for using Google Maps walking distance. Unfortunately, one has to use Google Maps API to run the code below, if you do not have one set, please just use the RDS file generated by the code. 

```{r}
wowa <- data.table( 
  read.csv("whole_wowa/whole_wowa.tsv", 
           sep="\t", 
           header = TRUE,
           stringsAsFactors = FALSE,
           fill = NA) 
  ) %>% 
  mutate(latitude = as.numeric(location1), 
         longitude = as.numeric(location2)) %>% 
  filter(!is.na(latitude) & 
           !is.na(weight) & 
           (pro != "wh") &
           (pro != "other"))
```




```{r}
set.api.key("YOUR GOOGLE MAPS API KEY")


# Slightly jitter northern+ankara to avoid complete overlap with Turkish from ankara

wowa[doculect == "northern+ankara"]$latitude <- 
  wowa[doculect == "northern+ankara"]$location1 + 0.05
wowa[doculect == "northern+ankara"]$longitude <- 
  wowa[doculect == "northern+ankara"]$location2 + 0.05

# The coordinates for Musandam are a bit inaccurate

wowa[doculect == "musandam"]$latitude[1] <- 26.00
wowa[doculect == "musandam"]$longitude[1] <- 56.18


distances <- matrix(0, nrow = 24, ncol = 24)
colnames(distances) <- unique(wowa$doculect)
rownames(distances) <- unique(wowa$doculect)

for (i in 1:24){
  doc <- unique(wowa$doculect)[i]
  lat.i <- wowa[doculect == doc]$latitude[1]
  long.i <- wowa[doculect == doc]$longitude[1]
  coordinates.i <- paste(as.character(lat.i), as.character(long.i), sep = "+")
  for (j in 1:24){
    if (i != j){
      other_doc <- unique(wowa$doculect)[j]
      lat.j <- wowa[doculect == other_doc]$latitude[1]
      long.j <- wowa[doculect == other_doc]$longitude[1]
      coordinates.j <- paste(as.character(lat.j), as.character(long.j), sep = "+")
      distances[i ,j] <- gmapsdistance(origin = coordinates.i,
                                       destination = coordinates.j,
                                       mode = "walking")$Distance 
    }
  }
}

# Note that the distances are measured in metres initially, this will be changed later

#saveRDS(distances, "walking_distances.rds")
```

The next thing we need for the analysis is the phylogenetic distance between the doculects. To get it, I will use the package "glottoTrees" by Erich Round which allows one to handle phylogenetic trees and, more importantly, to extract them from Glottolog. Note though that many of the languages used in this study represent varieties that previously have not been documented and thus, we cannot find an exact correspondence between all the languages in the study and in Glottolog. Nevertheless, we are only interested in relative position of one doculect with respect to others, this would allow us to extract good enough phylogenetic distances to work with. For that, I have created a file with the doculects that we have in WOWA dataset and the Glottocodes that I will use to build the phylogenetic distance matrix. 


```{r}
# file with the correspondences 
glotto_correspond <- read.csv("glottolog_correspondences.csv", sep = ";")
doculects <- glotto_correspond$Glottocode
```

```{r}
# we get the whole Glottolog dataset and keep only the relevant doculects from it
glotto <- data.table(get_glottolog_languages())
wowa_glotto <- glotto[glottocode %in% doculects]

big_tree <- abridge_labels(
  get_glottolog_trees(c("Indo-European", "Afro-Asiatic", 
                           "Kartvelian", "Turkic"))
)

big_tree <- assemble_rake(big_tree)
big_tree_rescaled <- geiger::rescale(big_tree, model = "delta", 0.35) # see the comment below

wowa_tips <- wowa_glotto$glottocode

all_wowa <- keep_tip(big_tree_rescaled, wowa_tips)
wowa_names <- as.character(glotto_correspond[,1])
glotto_wowa <- as.character(glotto_correspond[,2])

# the following loop is needed just to rename the tips with their name in WOWA dataset
for (i in 1:length(all_wowa$tip.label)){
  tip_glotto <- all_wowa$tip.label[i]
  index <- match(tip_glotto, glotto_wowa)
  all_wowa$tip.label[i] <- wowa_names[index]
}

# saveRDS(all_wowa, "df_phylo_glotto_delta0_35.rds")

plot(all_wowa)
```

Since all the branches of the tree that we initially build from Glottolog data are equal to one, we need to do some rescaling, otherwise the tree does not look like any of the trees that have been previously inferred based on lexical data A few words about the rescaling that I chose. In a recent pre-print by Macklin-Cordes and Round (https://arxiv.org/abs/2201.00195), they recommend rescaling the branch lengths in exponential fashion by using function rescale_branches_exp() provided by their package. The branch lengths it yields for Kurdish and Oghuz varieties are seemingly too short. Since there is not much research on optimal rescaling in case of linguistics, I decided to use rescale() function from "geiger" package (https://www.rdocumentation.org/packages/geiger/versions/2.0.7/topics/rescale). The branch lengths rescaled in this fashion do not become short as quickly as in case of rescale_branches_exp() but do look similar, which I will show in the following code section. The number of delta = 0.35 is somewhat arbitrary. Anyways, using no rescaling at all would probably be a bigger error. In the following code section, I repeat almost entirely the procedure but this time apply rescale_branches_exp() for comparison. 

```{r}
big_tree_exp <- rescale_branches_exp(big_tree)

all_wowa_exp <- keep_tip(big_tree_exp, wowa_tips)
wowa_names <- as.character(glotto_correspond[,1])
glotto_wowa <- as.character(glotto_correspond[,2])

# the following cycle is needed just to rename the tips with their name in WOWA dataset
for (i in 1:length(all_wowa_exp$tip.label)){
  tip_glotto <- all_wowa_exp$tip.label[i]
  index <- match(tip_glotto, glotto_wowa)
  all_wowa_exp$tip.label[i] <- wowa_names[index]
}

# make visual comparison for now
plot(all_wowa_exp)
```

Finally, for comparison I will also create a tree with no scaling at all and then compare the distances between the three trees via ape's function dist.topo().

```{r}

all_wowa_no <- keep_tip(big_tree, wowa_tips)
wowa_names <- as.character(glotto_correspond[,1])
glotto_wowa <- as.character(glotto_correspond[,2])

# the following cycle is needed just to rename the tips with their name in WOWA dataset
for (i in 1:length(all_wowa_no$tip.label)){
  tip_glotto <- all_wowa_no$tip.label[i]
  index <- match(tip_glotto, glotto_wowa)
  all_wowa_no$tip.label[i] <- wowa_names[index]
}

plot(all_wowa_no)
```



```{r}
dist.topo(c(all_wowa, all_wowa_exp, all_wowa_no), method = "score")
```

As we can see, the tree that I will use for estimating phylogenetic distance (tree1 in the output) and building autocorrelation matrix between the doculects is somewhere in the middle between unscaled tree3 and exponentially scaled tree2. The only justification for this kind of tree is that it implies a little larger distances for very closely related doculects (like Kurdish varieties) because otherwise, if one computes a correlation matrix for exponentially scaled tree using ape's function vcv.phylo(), we can see that because of rounding error, some varieties get a value of 1 off the diagonal, which is not optimal for the Gaussian process that I will use in the main model. 

I will use the correlation matrix as a scaled genetic distance matrix between the doculects. Note that the covariance between two taxa on a tree are defined by using the branch lengths and correlation is in turn the scaled covariance. The values of covariance are defined on the interval from negative infinity to positive infinity, while correlation is defined between the interval of -1 to 1.


```{r}
phylo.corr <- vcv.phylo(all_wowa_exp, corr = TRUE)
ggcorrplot(phylo.corr, lab = TRUE)
```

At the same time, the delta rescaled tree looks a bit more realistic in terms of its correlation. The highest values off the diagonal are now equal to 0.94-0.95 which looks plausible. This means that doculects have some space to vary because they are not perfectly correlated with some other varieties but should also be similar enough because of their relatively recent divergence.

```{r}
phylo.corr <- vcv.phylo(all_wowa, corr = TRUE)
ggcorrplot(phylo.corr, lab = TRUE)
```

Since we are working with distance measure, we need to make the closely related doculects to have smaller values (i.e. lower genetic distance) and all the values on the diagonal to be zero (the distance between a doculect and itself is 0). I use a simple procedure: subtract 1 from every cell and take the absolute value of the number. 

```{r}
### If you have not run the previous chunks, uncomment the following line

# all_wowa <- readRDS("df_phylo_glotto_delta0_35.rds") 
phylo.corr <- vcv.phylo(all_wowa, corr = TRUE)
```


```{r}
mod_phylo.corr <- matrix(0, nrow(phylo.corr), ncol(phylo.corr))
colnames(mod_phylo.corr) <- colnames(phylo.corr)
rownames(mod_phylo.corr) <- rownames(phylo.corr)
for (i in 1:nrow(phylo.corr)){
  for (j in 1:ncol(phylo.corr)){
    mod_phylo.corr[i, j] <- abs(phylo.corr[i, j] - 1)
  }
}

ggcorrplot(mod_phylo.corr, lab = TRUE)
```

Now I will load again the geographic distance matrix. Note that the initial distance matrix was saved and used meters as distance. I will thus load the matrix and divide it by a million to get the distance in thousands of kilometers

```{r}
### If you have not run the previous chunks, uncomment the following line

# distances <- readRDS("walking_distances.rds")
geodistance <- distances/1000000
```

To make sure we have both matrices ordered in the same way, I use the following piece of code:

```{r}
mod_phylo.corr <- mod_phylo.corr[order(rownames(mod_phylo.corr)), 
                                 order(colnames(mod_phylo.corr))]

geodistance <- geodistance[order(rownames(geodistance)), 
                           order(colnames(geodistance))]

```

Due to number of digits after comma, some numbers slightly differ. The difference is minor but the matrices should be symmetric for Gaussian process to work properly. To deal with it, I round the number to 3 digits after comma and then copy the values so that cell with indices i and j is equal in its value to the cell with indices j and i.

```{r}
for (i in 1:length(colnames(geodistance))){
  for (j in 1:length(colnames(mod_phylo.corr))){
    geodistance[i, j] <- round(geodistance[j, i], digits = 3)
    mod_phylo.corr[i, j] <- round(mod_phylo.corr[j, i], digits = 3)
  }
}
```

```{r}
ggcorrplot(geodistance, lab = TRUE, show.legend = FALSE)
ggcorrplot(mod_phylo.corr, lab = TRUE, show.legend = FALSE)
```

Now I load the WOWA dataset once again and transform some of the variables. Since I would like to have an interaction between two categorical variables, role and flagging, I will make separate indices for them, so that the coefficients are more easily interpretable. 

```{r}
wowa <- data.table( 
  read.csv("whole_wowa/whole_wowa.tsv", sep="\t", header=T,
           stringsAsFactors=FALSE,
           fill=NA)) %>% 
  mutate(role_simple = ifelse((role == "do" | role == "do-def"), "do", "non-do"),
             anim_simple = ifelse(anim == "hum", "hum", "non-hum"), 
             flag_simple = ifelse(flag == "bare", "bare", "non-bare"),
             pro_simple = ifelse(pro == "", "NP", "pro"),
             latitude = as.numeric(location1), 
             longitude = as.numeric(location2),
             family = affiliation1) %>% 
  mutate(role_mod = 
           ifelse(
             (role == "do" | role == "do-def" | role == "poss"), "do", ifelse(
                 (role == "goal" | role == "goal-c"), "goal", ifelse(
                   (role == "cop"), "cop", ifelse(
                     (role == "becm" | role == "becm-c"), "becm", "other"
                   )
                 )
             )
           )
  ) %>% 
  # there is a language with missing coordinates in the dataset
  # we are not interested in questions and language specific referential devices (pro != "other")
  filter(!is.na(latitude) & 
           !is.na(weight) & 
           (pro != "wh") &
           (pro != "other"))  %>% 
  # I could have made a matrix and passed it to STAN but I learned about this method after writing this piece of code
  # So, to keep everything transparent, I keep the ugly code but that is the one I actually used 
  mutate(role_flag = 
           ifelse(
             (role_mod == "do" & flag_simple == "bare"), "do_bare", ifelse(
               (role_mod == "do" & flag_simple == "non-bare"), "do_non", ifelse(
                     (role_mod == "goal" & flag_simple == "bare"), "goal_bare", ifelse(
                       (role_mod == "goal" & flag_simple == "non-bare"), "goal_non", ifelse(
                         (role_mod == "cop" & flag_simple == "bare"), "cop_bare", ifelse(
                           (role_mod == "cop" & flag_simple == "non-bare"), "cop_non", ifelse(
                             (role_mod == "becm" & flag_simple == "bare"), "becm_bare", ifelse(
                               (role_mod == "becm" & flag_simple == "non-bare"), "becm_non", ifelse(
                                 (role_mod == "other" & flag_simple == "bare"), "other_bare", "other_non"
                               )
                            )
                         )
                       )
                     )
                   )
                 )
               )
             )
           )
```


There may be some variation between speakers that we would like to account for. Since not all the datasets contain explicit information on which texts were produced by which speaker, I will use text IDs as a proxy for that. To do that, we have to assign every text a unique ID. The following code does that. 

```{r}
doculects <- wowa$doculect
textID <- wowa$textID

newID <- c(1)
indexID <- 1

for (i in 2:length(doculects)){
  if (doculects[i] == doculects[i-1] & textID[i] == textID[i-1]){
    newID <- c(newID, indexID)
  } else {
    indexID <- indexID + 1
    newID <- c(newID, indexID)
  }
}

wowa$new_textID <- newID
```

Now I will create a list with all the variables and their values that will be passed to STAN through "rethinking" interface. I will only use 70% of the data for training and 30% for prediction. Thus, we will be able to see how the model performs on previously unseen data. Note that I include additional data which is unused in the model (the model itself is below the following chunk). Afterwards during the work on the term paper, I will run a few other models to determine whether there is any additional value in adding other predictors and compare possible causal models underlying the generative process. The seed is included to ensure reproducibility but can in principle be changed. 


```{r}
set.seed(6871)
index <- sample(1:nrow(wowa), round(0.7*nrow(wowa), digits = 0))
wowa_sample <- wowa[index,] # 70% training
wowa_sample_pred <- wowa[-index,] # 30% prediction/testing


data_70sample <- with(wowa_sample, list(
  weight = weight,
  pos = position, 
  pro = as.factor(pro_simple), 
  flag = as.factor(flag_simple),
  role = as.factor(role_mod), 
  anim = as.factor(anim_simple),
  geo_dist = geodistance, 
  phylo = mod_phylo.corr, 
  doculect = as.factor(doculect), 
  num_doc = length(unique(doculect)), 
  subfam = as.factor(affiliation2), 
  role_flag = as.factor(role_flag), 
  textID = as.factor(new_textID)
))


data_30sample <- with(wowa_sample_pred, list(
  weight = weight,
  pos = position, 
  pro = as.factor(pro_simple), 
  flag = as.factor(flag_simple),
  role = as.factor(role_mod), 
  anim = as.factor(anim_simple),
  geo_dist = geodistance, 
  phylo = mod_phylo.corr, 
  doculect = as.factor(doculect), 
  num_doc = length(unique(doculect)), 
  subfam = as.factor(affiliation2), 
  role_flag = as.factor(role_flag),
  textID = as.factor(new_textID)
))
```

The model presented in this Rmarkdown has the following structure (priors' selection and kernels described below):

$$
position \sim \text{Binomial}(1, p) \\
\text{logit}(p) = phylodistance + geodistance + role_i\cdot flag_i + textID_i + \beta_w\cdot weight_i \\
phylodistance \sim
\text{MVNormal}
  \begin{pmatrix} 
  \begin{pmatrix} 0\\ 0 \\ \ldots \\ 0\end{pmatrix}, 
  \text{K}_{phylo}
  \end{pmatrix}
\\
geodistance \sim 
\text{MVNormal}
  \begin{pmatrix} 
  \begin{pmatrix} 0\\ 0 \\ \ldots \\ 0\end{pmatrix}, 
  \text{K}_{geo}
  \end{pmatrix}
\\
\text{K}_{phylo} = \eta^2\exp(-\rho^2D_{i,j}) \\
\text{K}_{geo} = \eta^2\exp(-\rho^2D_{i,j}^2) + \delta_{i,j}\sigma^2 \\
role_i\cdot flag_i \sim \text{Normal}(0, 1.5) \\
textID_i \sim \text{Normal}(-1, 0.5) \\
\beta_w \sim \text{Normal}(0, 1.5)
$$

The priors selected here are mostly uninformative and uniform, within reasonable boundaries. Since we have more than 10000 data points, that should pose no problem to the model, although the computation will be a little slower than possible. I only use the interaction term between role and flag because in some languages there are big differences between bare (flagged) and non-bare constituents. Because of it, it is doubtful whether there is some sense in attributing the role or the flag some effect on their own. I have also tested a model without this interaction term, where role and flag are included in the model separately and Watanabe-Akaike Information Criteria (WAIC) clearly showed support for the model with the interaction term. Since we have a logistic regression, it is worth looking at transformed values, rather than absolute numbers. So, to understand what the priors above mean, I will plot three distributions which will show the values on probability scale, rather than absolute scale. That means, that every value will show how much more likely it will affect the change from position 0 (pre-verbal position) to 1 (post-verbal position). We start with role and flag interaction

```{r}
role_flag_prior <- inv_logit(
  rnorm(1e5, 0, 1.5)
)

ggplot() + 
  geom_density(aes(x = role_flag_prior))
```

As we can see, the values are constrained between 0 and 1, since the x-axis now shows the value of a parameter in probabilites, i.e. how much more probable it becomes to have post-verbal element, given a particular combination of role and flag values. Note that the distribution nicely captures that we are skeptical about extreme values closer to 0 and 1 and do not have any special preference for the values in the middle. 

```{r}
text_prior <- inv_logit(
  rnorm(1e5, -1, 0.5)
)

ggplot() + 
  geom_density(aes(x = text_prior))
```

It might be that in this case, the prior is actually broader than needed and I might later constraint it to even lower values. The text ID is just a proxy for individual variation, so we definitely want it to be small and have only marginal effect on the outcome. That's why the distribution is biased to have lower values, although high individual variation is also possible, as there are quite a lot of values over 0.4. 

Finally, the effect of weight is included and it might be relatively strong but might also minor importance. In fact, Heavy NP shift is a phenomenon well observed in case of some languages, whereas its effect is not as clear in case of the others. Thus, we are certainly interested in the effect strength. I might have biased it to have slightly higher values but again, we could simultaneously test many hypotheses (not in the NHST sense) and because of it, I avoid giving $\beta_w$ a biased value towards the upper end. 

```{r}
beta_w <- inv_logit(
  rnorm(1e5, 0, 1.5)
)

ggplot() + 
  geom_density(aes(x = beta_w))
```

The choice of priors for Gaussian processes is also justified in the same manner, we want them to be wide enough to let the data inform the posterior distribution. There is one downside of the model that I will correct at later stages. Since we have multiple observations for every language, I should have not set the value of sigma in Gaussian process to a small constant (the last argument in functions cov_GPL1 and cov_GPL2), rather it would be better to make it a parameter and additionally infer it. That would allow the strength of the effect to vary and might provide a better fit. 
Finally, note that I have used non-centered parametrization for Gaussian processes, since that improves the fitting time and also allows us to get a higher effective number of samples. Note that because of the complexity, I specified a higher max_treedepth (default = 10) and adapt_delta (default = 0.95) and also increased number of iterations to 8000. This allows to get a reasonable number of effective number of samples (>300) and we also get the Rhat4 values equal to or lower than 1.02. 

NOTE: The model might take a lot of time to process (>6 hours on an average laptop)

```{r}
set.seed(92475)
phylo.OU_geo.brown_text.prior_70sample <- ulam(
  alist( 
    pos ~ binomial(1, p), 
    logit(p) <- phy[doculect] + geo[doculect] +
      rol_flagging[role_flag] + text[textID] +
      beta_w*weight, 
    
    rol_flagging[role_flag] ~ normal(0, 1.5),
    
    text[textID] ~ normal(-1, 0.5),
    
    beta_w ~ normal(0, 1.5), 
    
    transpars> vector[num_doc]: phy <<- phy_Sigma*phy_z,
    vector[num_doc]: phy_z ~ normal(0, 1),
    transpars> matrix[num_doc, num_doc]: phy_Sigma <<- cholesky_decompose(phy_SIGMA), 
    transpars> matrix[num_doc, num_doc]: phy_SIGMA <- cov_GPL1(phylo, etasq, rhosq, 0.01),
    etasq ~ half_normal(1,0.25),
    rhosq ~ half_normal(3,0.25),
    
    transpars> vector[num_doc]: geo <<- geo_Sigma*geo_z,
    vector[num_doc]: geo_z ~ normal(0, 1),
    transpars> matrix[num_doc, num_doc]: geo_Sigma <<- cholesky_decompose(geo_SIGMA),
    transpars> matrix[num_doc, num_doc]: geo_SIGMA <- cov_GPL2(geo_dist, etasq_d, rhosq_d, 0.01),
    etasq_d ~ exponential(2), 
    rhosq_d ~ exponential(0.75)
    
  ), data = data_70sample, chains = 2, cores = 8, iter = 8000,
  control=list(adapt_delta=0.99, max_treedepth = 12)
)


# saveRDS(phylo.OU_geo.brown_text.prior_70sample, "phylo.OU_geo.brown_text.prior_70sample.rds")

### If you haven't run the model, please use the following line
# phylo.OU_geo.brown_text.prior_70sample <- readRDS("phylo.OU_geo.brown_text.prior_70sample.rds")
```

To inspect the effective number of samples and Rhat4 values, please use the following piece of code

```{r}
precis(phylo.OU_geo.brown_text.prior_70sample, depth = 2)
```

Since the total number of parameters of the model is extremely high, please use "pars" argument to only inspect the ultimate variables that contain the values we are interested in. The rest of the variables are intermediary variables used for non-centered parametrization. 

```{r}
trankplot(phylo.OU_geo.brown_text.prior_70sample, 
          pars = c("phy", "geo", "text", "beta_w", 
                   "rol_flagging", "etasq", "rhosq", 
                   "etasq_d", "rhosq_d"))
```


To analyze the predictions made by the model, I will use the mean value of each posterior distribution for the value of p. For predictions, I will use the 30% of the data that were not included in the training dataset. In the end of the code, I display the percentage of predictions that differ strongly from the real value of p. To do that, I subtract the position value (0 or 1) from the mean of the posterior distribution (takes real value and ranges between 0 and 1) and take the absolute value of this difference. For example, if the real position of an element is post-verbal (=1), while the predicted mean value of p is equal to 0.2, the absolute difference is 0.8 and I count it as a poor prediction, since the model expects the element to be more likely pre-verbal. Overall, you can see that according to this metric, we have around 14% of poor predictions.  

```{r}

p_link <- link(phylo.OU_geo.brown_text.prior_70sample, data = list(
  weight = data_30sample$weight,
  pos = data_30sample$pos, 
  pro = data_30sample$pro, 
  flag = data_30sample$flag,
  role = data_30sample$role,
  anim = data_30sample$anim,
  geo_dist = data_30sample$geo_dist, 
  phylo = data_30sample$phylo, 
  doculect = data_30sample$doculect, 
  num_doc = data_30sample$num_doc, 
  pro_mod = data_30sample$pro_mod, 
  textID = data_30sample$textID, 
  role_flag = data_30sample$role_flag
))

p_link_mean <- apply(p_link, 2, mean)

wowa_sample_pred$predict_mean <- p_link_mean

wowa_sample_pred[abs(predict_mean - position) > 0.5, .N]/wowa_sample_pred[, .N]
```

These poor predictions are more frequent in some languages and also note that the number of data points for some doculects is much higher than for other. Because of it, I display the number of poor predictions and the total number of data points for each doculect. 

```{r}
miss_pred <- wowa_sample_pred[abs(predict_mean - position) > 0.5, .(miss = .N), by = doculect]
total_points <- wowa_sample_pred[, .(total = .N), by = doculect]

prop <- merge.data.table(miss_pred, total_points)
#prop$proportion <- round(prop$miss/prop$total, digits = 2)

prop <- data.table(melt(prop))
prop <- prop[order(prop$value, decreasing = T)]

ggplot(prop, aes(x = reorder(doculect, -value), y = value, fill = variable)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme(axis.text.x = element_text(angle = 90)) +
  ylim(0, 460) +
  xlab("")
```


Even though it seems that a lot of elements are poorly predicted for Coastal Balochi, it is also the doculect with the highest number of data points and proportionally, the model does a good job in predicting the position of elements in Coastal Balochi. Conversely, we can see that there is a high proportion of poorly predicted elements' positions in Gagauz and Romeyka. This is not surprising, since in case of Gagauz we do not have Russian or Romanian (both SVO languages) to show the contact influence of these languages on Gagauz, while in case of Romeyka, we do not have any other Hellenic languages (e.g. standard Greek or Cypriot Greek) to show that the language was initially SVO and because of contact it switched to being mixed SVO and SOV. 

Also note another issue with this metric. For some languages, it is valid to say that element may either occupy pre-verbal or post-verbal position freely or for the reasons that are beyond our dataset (e.g. discourse factors) and the model should then predict the values close to 0.5. In fact, this is what happens and also note that the distribution of actual position is almost perfectly random. 

```{r}
wowa_sample_pred[predict_mean > 0.4 & predict_mean < 0.6, .N, by = position]
```

Thus, if we instead choose to only count the values with difference over 0.6 as poor predictions, then the model poorly predicts only 10% of the new data.

```{r}
wowa_sample_pred[abs(predict_mean - position) > 0.6, .N]/wowa_sample_pred[, .N]

```

I will plot the predictions now. It is not a trivial task, so a short description about the way one should interpret the following plots. On the y-axis, the possible values are only 0 and 1. This is the position that an element takes in the dataset Because of that, all the elements should actually be aligned along two horizontal lines but this would make it difficult to see how many points have value of 0 and 1. Because of it, I used geom_jitter() and thus slightly shifted all the points from the horizontal line. I set the points to be minimally shifted along x-axis in order to reflect the inferred mean values of p correctly. To reiterate the last idea, on the x-axis we have the inferred mean values of posterior distribution of p for each element. If you think of the way the plot is organized, we should ideally have all the points in the lower left square (the predicted value of p is low and the actual position of the element is 0) or in the upper right square (the predicted value of p is high and the actual position of the element is 1). The colors reflect different roles, while the shape of a data point is reflecting whether it is bare or non-bare element. 

```{r}

ggplot(wowa_sample_pred) +
  geom_jitter(aes(x = predict_mean, 
                  y = position, 
                  color = role_mod, 
                  shape = flag_simple),
              na.rm = TRUE, size = 3.5, 
              width = 0.05) +
  facet_wrap(~affiliation2 + doculect) +
  geom_hline(yintercept = 0.5) +
  geom_vline(xintercept = 0.5) +
  ylim(0, 1) +
  xlim(0, 1) +
  labs(color = "Role:", shape = "Flagging:", size = 2) +
  labs(x = "Predicted value of *p*", 
       y = "Real position") +
  theme_bw() +
  theme(legend.position="bottom",
        axis.title.x = ggtext::element_markdown(), 
        legend.text=element_text(size=10.5)) +
  ggtitle("Fig. 1: All the predictions on a sample of data")


```


Since the category "other" is essentially a mix of different categories, it is to be expected that it would account for many poor predictions. The plot is essentially the same but now we remove the category "other".

```{r}
ggplot(wowa_sample_pred[role_mod != "other"]) +
  geom_jitter(aes(x = predict_mean, 
                  y = position, 
                  color = role_mod, 
                  shape = flag_simple),
              na.rm = TRUE, size = 3.5, 
              width = 0.05) +
  facet_wrap(~affiliation2 + doculect) +
  geom_hline(yintercept = 0.5) +
  geom_vline(xintercept = 0.5) +
  ylim(0, 1) +
  xlim(0, 1) +
  labs(color = "Role:", shape = "Flagging:", size = 2) +
  labs(x = "Predicted value of *p*", 
       y = "Real position") +
  theme_bw() +
  ggtitle("Fig. 2: Removing the category 'other' ", 
          subtitle = ) +
  theme(legend.position="bottom",
        axis.title.x = ggtext::element_markdown(), 
        legend.text=element_text(size=10.5))

```

We expect the non-bare elements to have higher flexibility and to be able to change its position because of the flagging. So, now I keep only "bare" elements without the category "other" and now the shape reflects the referential form an element (noun phrase versus pronoun). Note how most of the poorly predicted positions of direct objects in Kumzari (Musandam) are explained by pronominal form of direct object. Kumzari is one of the few languages that in fact breaks the Greenberg Universal #25: "If the pronominal object follows the verb, so does the nominal object". Most likely this happened under the influence of Arabic but note that we lack Arabic varieties from the Arabian peninsula and because of it, the influence of Arabic is probably underestimated. In the background I have tried to fit a model with the interaction term between the form (pro/np) and the subfamily but the model wouldn't sample well. Because of it, some other way to account for it has to be found. 

```{r}

ggplot(wowa_sample_pred[flag_simple == "bare" & role_mod != "other"]) +
  geom_jitter(aes(x = predict_mean, 
                  y = position, 
                  color = role_mod, 
                  shape = pro_simple),
              na.rm = TRUE, size = 3.5, 
              width = 0.05) +
  facet_wrap(~affiliation2 + doculect) +
  geom_hline(yintercept = 0.5) +
  geom_vline(xintercept = 0.5) +
  ylim(0, 1) +
  xlim(0, 1) +
  labs(color = "Role:", shape = "", size = 2) +
  labs(x = "Predicted value of *p*", 
       y = "Real position") +
  theme_bw() +
  theme(legend.position="bottom",
        axis.title.x = ggtext::element_markdown(), 
        legend.text=element_text(size=10.5)) +
  ggtitle("Fig. 3: All the elements are bare", subtitle = "Shape is now 'pro' vs 'NP'")

```

The next steps in this analysis (not included here) would have to include the comparison of different Kernels for geographic and phylogenetic distances via model comparison procedure (Leave-one-out and WAIC), the use of different variables (including animacy vs. excluding it). Including animacy is seemingly redundant, as the role of an element already contains some information about it. Because of it, I thought that the causal structure that we find in this case is a pipe (animacy -> role -> position) and I thought that the animacy should be excluded from the analysis, although it is hard to decide. It might as well be that animacy and role merely share a common cause.

Finally, I have some misspecifications in the model presented above. For instance, I will certainly add the parameter sigma to the quadratic kernel used for geographic distance, which is currently a constant. Apart from that, maybe different priors for Gaussian processes should be tried because currently they might be too wide. 

I could not correct and include those corrections as my Stan broke, after I had tried to install a new cmdstanr version and the new version of rethinking package. I will eventually solve the problem and complete the work but it unfortunately limited me in preparing the whole analysis for the application properly. 





